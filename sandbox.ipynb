{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "801cf492-2a8a-4ae8-bd23-f65489baa7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import requests\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    ShortSideScale,\n",
    "    UniformTemporalSubsample,\n",
    ")\n",
    "import csv\n",
    "from io import StringIO\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from pytorchvideo.data.encoded_video import EncodedVideo\n",
    "from torchvision.transforms._transforms_video import NormalizeVideo\n",
    "from transforms import SpatialCrop, TemporalCrop, DepthNorm\n",
    "import cv2\n",
    "import glob\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "aa0da705-4cae-48ae-a80f-168593c28b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f9e53739-c215-4032-8024-137172ce4637",
   "metadata": {},
   "outputs": [],
   "source": [
    "def files_to_video(files):\n",
    "    video_name = './video.mp4'\n",
    "    frame = Image.open(files[0])\n",
    "    video = cv2.VideoWriter(video_name, cv2.VideoWriter_fourcc(*'mp4v'), 24, (frame.width, frame.height))\n",
    "    for file in files:\n",
    "        video.write(cv2.imread(file))\n",
    "    video.release()\n",
    "    # return video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "29c1e189-e2c7-407d-b842-a809375e6cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_to_video(sorted(glob.glob('./satis-cv-ai-exercise-data/train/P01/P01_05/*.jpg')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0a4e59a0-8b3c-4b6d-b27e-4b13f5107107",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/vshmyhlo/.cache/torch/hub/facebookresearch_omnivore_main\n"
     ]
    }
   ],
   "source": [
    "model_name = \"omnivore_swinB_epic\"\n",
    "model = torch.hub.load(\"facebookresearch/omnivore:main\", model=model_name)\n",
    "\n",
    "# Set to eval mode and move to desired device\n",
    "model = model.to(device)\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "758d1db1-0aaf-4c61-938c-49a04fd5e02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get('https://dl.fbaipublicfiles.com/omnivore/epic_action_classes.csv')\n",
    "reader = csv.reader(StringIO(response.text))\n",
    "epic_id_to_action = {idx: \" \".join(rows) for idx, rows in enumerate(reader)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eaf54a37-36b2-46aa-ad80-c512d7dafde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_frames = 32\n",
    "sampling_rate = 2\n",
    "frames_per_second = 30\n",
    "\n",
    "clip_duration = (num_frames * sampling_rate) / frames_per_second\n",
    "\n",
    "video_transform = ApplyTransformToKey(\n",
    "    key=\"video\",\n",
    "    transform=T.Compose(\n",
    "        [\n",
    "            UniformTemporalSubsample(num_frames), \n",
    "            T.Lambda(lambda x: x / 255.0),  \n",
    "            ShortSideScale(size=224),\n",
    "            NormalizeVideo(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            TemporalCrop(frames_per_clip=32, stride=40),\n",
    "            SpatialCrop(crop_size=224, num_crops=3),\n",
    "        ]\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4b58fa5a-a68d-463d-bc4d-2d185fb6c7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = \"./video.mp4\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a773d06b-27b4-4943-b78f-ddeda15169c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n"
     ]
    }
   ],
   "source": [
    "# Initialize an EncodedVideo helper class\n",
    "video = EncodedVideo.from_path(video_path)\n",
    "\n",
    "# Load the desired clip\n",
    "video_data = video.get_clip(start_sec=0.0, end_sec=2.0)\n",
    "\n",
    "# Apply a transform to normalize the video input\n",
    "video_data = video_transform(video_data)\n",
    "\n",
    "# Move the inputs to the desired device\n",
    "video_inputs = video_data[\"video\"]\n",
    "\n",
    "# Take the first clip \n",
    "# The model expects inputs of shape: B x C x T x H x W\n",
    "video_input = video_inputs[0][None, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "cf838f30-2238-4191-9947-c9d7a1b53516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 predicted actions: take bread, take bag, put bread, insert bread, put bag\n"
     ]
    }
   ],
   "source": [
    "# Pass the input clip through the model \n",
    "with torch.no_grad():\n",
    "    prediction = model(video_input.to(device), input_type=\"video\")\n",
    "\n",
    "    # Get the predicted classes \n",
    "    pred_classes = prediction.topk(k=5).indices\n",
    "\n",
    "# Map the predicted classes to the label names\n",
    "pred_class_names = [epic_id_to_action[int(i)] for i in pred_classes[0]]\n",
    "print(\"Top 5 predicted actions: %s\" % \", \".join(pred_class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d77a51-d998-4bd5-a7a5-0c30a6e2d710",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
